# AOT ID: ['0_forward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from cmath import nanj
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
assert_alignment = torch._C._dynamo.guards.assert_alignment
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()
empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p


def partition_0(args):
    primals_1, = args
    args.clear()
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        # Topologically Sorted Source Nodes: [linalg_eig], Original ATen: [aten.linalg_eig]
        buf0 = torch.ops.aten.linalg_eig.default(primals_1)
        del primals_1
        buf1 = buf0[0]
        assert_alignment(buf1, 16)
        buf2 = buf0[1]
        assert_alignment(buf2, 16)
        del buf0
        # Topologically Sorted Source Nodes: [], Original ATen: [aten._conj]
        buf3 = torch.ops.aten._conj.default(buf1)
        buf4 = buf3
        assert_alignment(buf4, 16)
    return (buf1, buf2, buf4, )


async_compile.wait(globals())
del async_compile

class Runner:
    def __init__(self, partitions):
        self.partitions = partitions

    def recursively_apply_fns(self, fns):
        new_callables = []
        for fn, c in zip(fns, self.partitions):
            new_callables.append(fn(c))
        self.partitions = new_callables

    def call(self, args):
        primals_1, = args
        args.clear()
        partition0_args = [primals_1,]
        del primals_1
        (buf1, buf2, buf4) = self.partitions[0](partition0_args)
        del partition0_args
        return (buf1, buf2, buf2, buf4, )

runner = Runner(partitions=[partition_0,])
call = runner.call
recursively_apply_fns = runner.recursively_apply_fns


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((5, 5), (5, 1), device='cuda:0', dtype=torch.float64)
    fn = lambda: call([primals_1])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)

# AOT ID: ['0_backward']
from ctypes import c_void_p, c_long, c_int
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from cmath import nanj
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align
from torch import device, empty_strided
from torch._inductor.async_compile import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels
from torch._inductor.codegen.multi_kernel import MultiKernelCall

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
_quantized = torch.ops._quantized
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
assert_alignment = torch._C._dynamo.guards.assert_alignment
empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
alloc_from_pool = torch.ops.inductor._alloc_from_pool
async_compile = AsyncCompile()
empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p


def partition_0(args):
    getitem_1, _conj_2, tangents_2, tangents_1 = args
    args.clear()
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0)
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.transpose]
        buf0 = torch.ops.aten.permute.default(getitem_1, [1, 0])
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.transpose]
        buf12 = torch.ops.aten.permute.default(getitem_1, [1, 0])
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.unsqueeze]
        buf20 = torch.ops.aten.unsqueeze.default(_conj_2, -1)
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.unsqueeze]
        buf22 = torch.ops.aten.unsqueeze.default(_conj_2, -2)
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.fill]
        buf26 = torch.ops.aten.full.default([5], 1.0, dtype=torch.complex128, layout=torch.strided, device=device(type='cuda', index=0), pin_memory=False)
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.transpose]
        buf38 = torch.ops.aten.permute.default(getitem_1, [1, 0])
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.transpose]
        buf44 = torch.ops.aten.permute.default(getitem_1, [1, 0])
        buf1 = buf0
        assert_alignment(buf1, 16)
        buf13 = buf12
        assert_alignment(buf13, 16)
        buf21 = buf20
        assert_alignment(buf21, 16)
        buf23 = buf22
        assert_alignment(buf23, 16)
        buf27 = buf26
        assert_alignment(buf27, 16)
        del buf26
        buf39 = buf38
        assert_alignment(buf39, 16)
        buf45 = buf44
        assert_alignment(buf45, 16)
        # Topologically Sorted Source Nodes: [], Original ATen: [aten._conj]
        buf2 = torch.ops.aten._conj.default(buf1)
        # Topologically Sorted Source Nodes: [], Original ATen: [aten._conj]
        buf14 = torch.ops.aten._conj.default(buf13)
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.sub]
        buf24 = torch.ops.aten.sub.Tensor(buf23, buf21)
        del _conj_2
        del buf20
        del buf21
        del buf22
        del buf23
        # Topologically Sorted Source Nodes: [], Original ATen: [aten._conj]
        buf40 = torch.ops.aten._conj.default(buf39)
        # Topologically Sorted Source Nodes: [], Original ATen: [aten._conj]
        buf46 = torch.ops.aten._conj.default(buf45)
        buf3 = buf2
        assert_alignment(buf3, 16)
        buf15 = buf14
        assert_alignment(buf15, 16)
        buf25 = buf24
        assert_alignment(buf25, 16)
        del buf24
        buf41 = buf40
        assert_alignment(buf41, 16)
        buf47 = buf46
        assert_alignment(buf47, 16)
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
        buf4 = torch.ops.aten.mm.default(buf3, tangents_2)
        del buf0
        del buf1
        del buf2
        del buf3
        del tangents_2
        # Topologically Sorted Source Nodes: [], Original ATen: []
        buf28 = torch.ops.aten.diagonal.default(buf25, 0, -2, -1)
        buf5 = buf4
        assert_alignment(buf5, 16)
        del buf4
        buf29 = buf28
        assert_alignment(buf29, 16)
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.diagonal]
        buf6 = torch.ops.aten.diagonal.default(buf5, 0, -2, -1)
        # Topologically Sorted Source Nodes: [], Original ATen: []
        buf30 = torch.ops.aten.copy_.default(buf29, buf27)
        del buf27
        del buf28
        del buf29
        buf7 = buf6
        assert_alignment(buf7, 16)
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.view_as_real]
        buf8 = torch.ops.aten.view_as_real.default(buf7)
        buf9 = buf8
        assert_alignment(buf9, 16)
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mul]
        buf10 = torch.ops.aten.mul.Tensor(getitem_1, reinterpret_tensor(buf9, (1, 5), (0, 12), 0))
        del buf6
        del buf7
        del buf8
        del buf9
        buf11 = buf10
        assert_alignment(buf11, 16)
        del buf10
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
        buf16 = torch.ops.aten.mm.default(buf15, buf11)
        del buf11
        del buf12
        del buf13
        del buf14
        del buf15
        buf17 = buf16
        assert_alignment(buf17, 16)
        del buf16
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.sub]
        buf18 = torch.ops.aten.sub.Tensor(buf5, buf17)
        del buf17
        del buf5
        buf19 = buf18
        assert_alignment(buf19, 16)
        del buf18
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.div]
        buf32 = torch.ops.aten.div.Tensor(buf19, buf25)
        del buf19
        del buf25
        buf33 = buf32
        assert_alignment(buf33, 16)
        del buf32
        # Topologically Sorted Source Nodes: [], Original ATen: []
        buf34 = torch.ops.aten.diagonal.default(buf33, 0, -2, -1)
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.mm]
        buf42 = torch.ops.aten.mm.default(buf33, buf41)
        del buf38
        del buf39
        del buf40
        del buf41
        buf35 = buf34
        assert_alignment(buf35, 16)
        buf43 = buf42
        assert_alignment(buf43, 16)
        del buf42
        # Topologically Sorted Source Nodes: [], Original ATen: []
        buf36 = torch.ops.aten.copy_.default(buf35, tangents_1)
        del buf33
        del buf34
        del buf35
        del tangents_1
        # Topologically Sorted Source Nodes: [], Original ATen: [aten._linalg_solve_ex]
        buf48 = torch.ops.aten._linalg_solve_ex.default(buf47, buf43)
        del buf43
        del buf44
        del buf45
        del buf46
        del buf47
        del getitem_1
        buf49 = buf48[0]
        assert_alignment(buf49, 16)
        del buf48
        # Topologically Sorted Source Nodes: [], Original ATen: [aten.view_as_real]
        buf53 = torch.ops.aten.view_as_real.default(buf49)
        buf54 = buf53
        assert_alignment(buf54, 16)
    return (buf54, )


async_compile.wait(globals())
del async_compile

class Runner:
    def __init__(self, partitions):
        self.partitions = partitions

    def recursively_apply_fns(self, fns):
        new_callables = []
        for fn, c in zip(fns, self.partitions):
            new_callables.append(fn(c))
        self.partitions = new_callables

    def call(self, args):
        getitem_1, _conj_2, tangents_1, tangents_2 = args
        args.clear()
        partition0_args = [getitem_1, _conj_2, tangents_2, tangents_1]
        del getitem_1, _conj_2, tangents_2, tangents_1
        (buf54,) = self.partitions[0](partition0_args)
        del partition0_args
        return (reinterpret_tensor(buf54, (5, 5), (2, 10), 0), )

runner = Runner(partitions=[partition_0,])
call = runner.call
recursively_apply_fns = runner.recursively_apply_fns


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    getitem_1 = rand_strided((5, 5), (5, 1), device='cuda:0', dtype=torch.complex128)
    _conj_2 = rand_strided((5, ), (1, ), device='cuda:0', dtype=torch.complex128)
    tangents_1 = rand_strided((5, ), (1, ), device='cuda:0', dtype=torch.complex128)
    tangents_2 = rand_strided((5, 5), (5, 1), device='cuda:0', dtype=torch.complex128)
    fn = lambda: call([getitem_1, _conj_2, tangents_1, tangents_2])
    return print_performance(fn, times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('None', benchmark_compiled_module)
